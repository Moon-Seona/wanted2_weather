{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "코드 공유에 올라와있는 EDA_공유.ipynb를 참고하여 전처리를 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 한글 폰트 사용 관련 설정\n",
    "# reference to https://velog.io/@winocas/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%8B%9C%EA%B0%81%ED%99%94-%ED%95%9C%EA%B8%80%EB%A7%88%EC%9D%B4%EB%84%88%EC%8A%A4-%EA%B9%A8%EC%A7%90-%EC%9C%88%EB%8F%84%EC%9A%B0\n",
    "\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "\n",
    "# 해당 셀 실행 후 런타임 다시 시작(단축키: Ctrl+M+.)을 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dataprep\n",
    "!pip install konlpy\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "\n",
    "# tokenizer 활용 위한 clone\n",
    "!git clone https://github.com/SKTBrain/KoBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./KoBERT/kobert_hf\")\n",
    "PATH = '../../../data/'\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "test = pd.read_csv(PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.loc[~train.duplicated(keep='first')].copy()\n",
    "test_data = test.loc[~test.duplicated(keep='first')].copy()\n",
    "\n",
    "drop_list = ['보안과제정보', '평가관리비', '0', '·', '보안사항으로 생략', '비밀과제로 보안상 생략', '비밀사업으로 보안상 생략']\n",
    "\n",
    "train_data = train_data.drop(train_data.query(\"요약문_연구내용 in @drop_list\").index)\n",
    "test_data = test_data.drop(test_data.query(\"요약문_연구내용 in @drop_list\").index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1', sp_model_kwargs={'nbest_size': -1, 'alpha': 0.6, 'enable_sampling': True})\n",
    "\n",
    "# tokenizer가 [unk]로 처리하는 특수문자 확인\n",
    "special_char = []\n",
    "col_names = ['요약문_연구목표', '요약문_연구내용', '요약문_기대효과']\n",
    "for col_name in col_names:\n",
    "    for text in train[col_name]:\n",
    "        spec_list = re.findall(\"[^가-힣a-zA-Z0-9\\n\\(^u) ]\", str(text))\n",
    "        for spe in spec_list:\n",
    "            if spe in special_char:\n",
    "                continue\n",
    "            else:\n",
    "                special_char.append(spe)\n",
    "            \n",
    "unk_spec_char = []\n",
    "for spe in special_char:\n",
    "    if tokenizer.unk_token_id in tokenizer.encode(spe):\n",
    "        if spe in unk_spec_char:\n",
    "            continue\n",
    "        else:\n",
    "            unk_spec_char.append(spe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = str(text)\n",
    "    # 특수문자를 모두 \\n으로 바꿔준다.\n",
    "    text = re.sub(\"(\\n)+(\\s)*\\W\",\"\\n\", text)\n",
    "\n",
    "    # 맨 앞에 남아있는 white space를 지워준다.\n",
    "    if re.match(\"\\w\", text) is None:\n",
    "        text = text[1:]\n",
    "\n",
    "    # \\n으로 구분되어있는 문자들을 잘라서 리스트에 넣어준다.\n",
    "    sentence_list = text.split(\"\\n\")\n",
    "    \n",
    "    # 잘린 문자열을 처리해서 상용 글자만 남겨둔다.\n",
    "    for sent_idx in range(len(sentence_list)):\n",
    "        re_sent = ''.join([char for char in sentence_list[sent_idx] if char not in set(unk_spec_char)])\n",
    "        # 맨 앞에 띄어쓰기가 남아있다면 지워준다.\n",
    "        if len(re_sent) > 0 and re_sent[0] == ' ':\n",
    "            re_sent = re_sent[1:]\n",
    "        # 리스트에 처리가 완료된 문자열을 넣어준다.\n",
    "        sentence_list[sent_idx] = re_sent\n",
    "    \n",
    "    # 하나의 리스트에 들어있는 문장들을 [SEP] 토큰으로 분류하여 하나의 문자열로 만들어준다.\n",
    "    pre_str = sentence_list.pop(0)\n",
    "    for s in sentence_list:\n",
    "        pre_str = pre_str + \" [SEP] \" + s\n",
    "    # 전처리 완료된 문자열 반환\n",
    "    return pre_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43364/43364 [09:29<00:00, 76.16it/s] \n",
      "100%|██████████| 43364/43364 [20:17<00:00, 35.61it/s]\n",
      "100%|██████████| 43364/43364 [11:53<00:00, 60.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keys = ['요약문_연구목표', '요약문_연구내용', '요약문_기대효과'] # 전처리 할 column\n",
    "# train data 전처리\n",
    "for key in (keys):\n",
    "    temp = []\n",
    "    for sent in tqdm(list(train_data[key])):\n",
    "        temp.append(text_preprocessing(sent))\n",
    "    train_data[key] = temp\n",
    "# test data 전처리\n",
    "for key in (keys):\n",
    "    temp = []\n",
    "    for sent in tqdm(list(test_data[key])):\n",
    "        temp.append(text_preprocessing(sent))\n",
    "    test_data[key] = temp\n",
    "print('FINISH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(PATH + 'train_preprocessing.csv', index=False)\n",
    "test_data.to_csv(PATH + 'test_preprocessing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EDA & KoBERT Tokenizer",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
